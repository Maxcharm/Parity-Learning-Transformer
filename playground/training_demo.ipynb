{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1df401aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/han/miniconda3/envs/fl/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "\n",
    "dataset = datasets.load_dataset(\"synthseq/flipflop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8bcaf8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "flip_flop_dict = {'0': 0, \"1\": 1, \"w\": 2,\"r\": 3, \"i\": 4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b983737b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def tokenize_raw(batch):\n",
    "    tokenized = [[flip_flop_dict[char] for char in s] for s in batch[\"text\"]]\n",
    "    return {\"tokens\": torch.tensor(tokenized, dtype=torch.int64)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "62c82a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.set_transform(tokenize_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ef1fb141",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_73661/1061149626.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  tokens = torch.tensor(tokens, dtype=torch.long)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class NextTokenDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, hf_dataset):\n",
    "        self.data = []\n",
    "        for item in hf_dataset:\n",
    "            tokens = item[\"tokens\"]\n",
    "            tokens = torch.tensor(tokens, dtype=torch.long)\n",
    "            x = tokens[:-1]\n",
    "            y = tokens[1:]\n",
    "            self.data.append((x, y))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx] \n",
    "\n",
    "train_small = dataset[\"train\"].select(range(2000))\n",
    "val_small = dataset[\"val\"].select(range(200))\n",
    "train_dataset = NextTokenDataset(train_small)\n",
    "val_dataset = NextTokenDataset(val_small)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b5074720",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class Sinusoidal_Embedding(nn.Module):\n",
    "    def __init__(self, embed_dim, max_len=512):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, embed_dim)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, embed_dim, 2) * (-math.log(10000.0) / embed_dim)\n",
    "        )\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer(\"pe\", pe) \n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.pe[:x.size(1)].unsqueeze(0).expand(x.size(0), -1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "208b4181",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            max_seq_len:int,\n",
    "            dictionary_size:int,\n",
    "            num_attn_layer:int=2,\n",
    "            num_attn_heads:int=2,\n",
    "            attn_dim:int=4,\n",
    "            \n",
    "        ):\n",
    "        super().__init__()\n",
    "        self.word_embedding = nn.Embedding(\n",
    "            num_embeddings=dictionary_size,\n",
    "            embedding_dim=attn_dim,\n",
    "            )\n",
    "        self.positional_embedding = Sinusoidal_Embedding(\n",
    "            embed_dim=attn_dim,\n",
    "            max_len=max_seq_len\n",
    "        )\n",
    "        self.attention_layers = [\n",
    "            nn.MultiheadAttention(\n",
    "                embed_dim=attn_dim,\n",
    "                num_heads=num_attn_heads,\n",
    "                batch_first=True,\n",
    "            ) \n",
    "            for _ in range(num_attn_layer)\n",
    "        ]\n",
    "        self.classification = [\n",
    "            nn.Linear(in_features=attn_dim, out_features=attn_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=attn_dim, out_features=dictionary_size),\n",
    "        ]\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        shape of x should be: (batch_size, seq_len)\n",
    "        \"\"\"\n",
    "        seq_len = x.size()[1]\n",
    "        word_emb = self.word_embedding(x)\n",
    "        pos_emb = self.positional_embedding(x)\n",
    "        emb = word_emb + pos_emb\n",
    "\n",
    "        attn_mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool().to(x.device)\n",
    "\n",
    "        for attn_layer in self.attention_layers:\n",
    "            emb, _ = attn_layer(emb, emb, emb, attn_mask=attn_mask)\n",
    "\n",
    "        \n",
    "        for layer in self.classification:\n",
    "            emb = layer(emb)\n",
    "\n",
    "        return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ea55b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train loss = 1.5250, val loss = 1.5243\n",
      "Epoch 2: train loss = 1.5248, val loss = 1.5240\n",
      "Epoch 3: train loss = 1.5237, val loss = 1.5210\n",
      "Epoch 4: train loss = 1.5162, val loss = 1.5051\n",
      "Epoch 5: train loss = 1.4749, val loss = 1.4438\n",
      "Epoch 6: train loss = 1.4271, val loss = 1.4105\n",
      "Epoch 7: train loss = 1.3991, val loss = 1.3868\n",
      "Epoch 8: train loss = 1.3789, val loss = 1.3699\n",
      "Epoch 9: train loss = 1.3642, val loss = 1.3570\n",
      "Epoch 10: train loss = 1.3525, val loss = 1.3458\n",
      "Epoch 11: train loss = 1.3408, val loss = 1.3316\n",
      "Epoch 12: train loss = 1.3156, val loss = 1.2963\n",
      "Epoch 13: train loss = 1.2799, val loss = 1.2647\n",
      "Epoch 14: train loss = 1.2555, val loss = 1.2456\n",
      "Epoch 15: train loss = 1.2379, val loss = 1.2292\n",
      "Epoch 16: train loss = 1.2224, val loss = 1.2146\n",
      "Epoch 17: train loss = 1.2085, val loss = 1.2015\n",
      "Epoch 18: train loss = 1.1959, val loss = 1.1894\n",
      "Epoch 19: train loss = 1.1843, val loss = 1.1783\n",
      "Epoch 20: train loss = 1.1734, val loss = 1.1678\n",
      "Epoch 21: train loss = 1.1633, val loss = 1.1578\n",
      "Epoch 22: train loss = 1.1537, val loss = 1.1485\n",
      "Epoch 23: train loss = 1.1445, val loss = 1.1397\n",
      "Epoch 24: train loss = 1.1359, val loss = 1.1313\n",
      "Epoch 25: train loss = 1.1279, val loss = 1.1235\n",
      "Epoch 26: train loss = 1.1202, val loss = 1.1162\n",
      "Epoch 27: train loss = 1.1132, val loss = 1.1092\n",
      "Epoch 28: train loss = 1.1064, val loss = 1.1027\n",
      "Epoch 29: train loss = 1.1001, val loss = 1.0965\n",
      "Epoch 30: train loss = 1.0940, val loss = 1.0906\n",
      "Epoch 31: train loss = 1.0883, val loss = 1.0849\n",
      "Epoch 32: train loss = 1.0826, val loss = 1.0794\n",
      "Epoch 33: train loss = 1.0773, val loss = 1.0742\n",
      "Epoch 34: train loss = 1.0723, val loss = 1.0692\n",
      "Epoch 35: train loss = 1.0674, val loss = 1.0644\n",
      "Epoch 36: train loss = 1.0626, val loss = 1.0595\n",
      "Epoch 37: train loss = 1.0575, val loss = 1.0546\n",
      "Epoch 38: train loss = 1.0529, val loss = 1.0501\n",
      "Epoch 39: train loss = 1.0485, val loss = 1.0457\n",
      "Epoch 40: train loss = 1.0441, val loss = 1.0413\n",
      "Epoch 41: train loss = 1.0396, val loss = 1.0367\n",
      "Epoch 42: train loss = 1.0350, val loss = 1.0320\n",
      "Epoch 43: train loss = 1.0301, val loss = 1.0270\n",
      "Epoch 44: train loss = 1.0252, val loss = 1.0219\n",
      "Epoch 45: train loss = 1.0198, val loss = 1.0165\n",
      "Epoch 46: train loss = 1.0145, val loss = 1.0110\n",
      "Epoch 47: train loss = 1.0091, val loss = 1.0057\n",
      "Epoch 48: train loss = 0.9959, val loss = 0.9786\n",
      "Epoch 49: train loss = 0.9701, val loss = 0.9630\n",
      "Epoch 50: train loss = 0.9630, val loss = 0.9600\n",
      "Epoch 51: train loss = 0.9603, val loss = 0.9573\n",
      "Epoch 52: train loss = 0.9576, val loss = 0.9548\n",
      "Epoch 53: train loss = 0.9553, val loss = 0.9525\n",
      "Epoch 54: train loss = 0.9530, val loss = 0.9503\n",
      "Epoch 55: train loss = 0.9509, val loss = 0.9482\n",
      "Epoch 56: train loss = 0.9490, val loss = 0.9463\n",
      "Epoch 57: train loss = 0.9471, val loss = 0.9444\n",
      "Epoch 58: train loss = 0.9452, val loss = 0.9426\n",
      "Epoch 59: train loss = 0.9435, val loss = 0.9408\n",
      "Epoch 60: train loss = 0.9416, val loss = 0.9391\n",
      "Epoch 61: train loss = 0.9400, val loss = 0.9374\n",
      "Epoch 62: train loss = 0.9383, val loss = 0.9358\n",
      "Epoch 63: train loss = 0.9366, val loss = 0.9343\n",
      "Epoch 64: train loss = 0.9352, val loss = 0.9328\n",
      "Epoch 65: train loss = 0.9338, val loss = 0.9313\n",
      "Epoch 66: train loss = 0.9324, val loss = 0.9299\n",
      "Epoch 67: train loss = 0.9309, val loss = 0.9285\n",
      "Epoch 68: train loss = 0.9295, val loss = 0.9271\n",
      "Epoch 69: train loss = 0.9281, val loss = 0.9258\n",
      "Epoch 70: train loss = 0.9267, val loss = 0.9245\n",
      "Epoch 71: train loss = 0.9254, val loss = 0.9232\n",
      "Epoch 72: train loss = 0.9242, val loss = 0.9220\n",
      "Epoch 73: train loss = 0.9230, val loss = 0.9208\n",
      "Epoch 74: train loss = 0.9218, val loss = 0.9196\n",
      "Epoch 75: train loss = 0.9206, val loss = 0.9184\n",
      "Epoch 76: train loss = 0.9195, val loss = 0.9173\n",
      "Epoch 77: train loss = 0.9184, val loss = 0.9162\n",
      "Epoch 78: train loss = 0.9171, val loss = 0.9151\n",
      "Epoch 79: train loss = 0.9162, val loss = 0.9140\n",
      "Epoch 80: train loss = 0.9151, val loss = 0.9129\n",
      "Epoch 81: train loss = 0.9140, val loss = 0.9119\n",
      "Epoch 82: train loss = 0.9130, val loss = 0.9108\n",
      "Epoch 83: train loss = 0.9119, val loss = 0.9098\n",
      "Epoch 84: train loss = 0.9109, val loss = 0.9088\n",
      "Epoch 85: train loss = 0.9099, val loss = 0.9078\n",
      "Epoch 86: train loss = 0.9089, val loss = 0.9068\n",
      "Epoch 87: train loss = 0.9079, val loss = 0.9058\n",
      "Epoch 88: train loss = 0.9070, val loss = 0.9049\n",
      "Epoch 89: train loss = 0.9060, val loss = 0.9039\n",
      "Epoch 90: train loss = 0.9051, val loss = 0.9030\n",
      "Epoch 91: train loss = 0.9040, val loss = 0.9021\n",
      "Epoch 92: train loss = 0.9032, val loss = 0.9012\n",
      "Epoch 93: train loss = 0.9022, val loss = 0.9003\n",
      "Epoch 94: train loss = 0.9014, val loss = 0.8994\n",
      "Epoch 95: train loss = 0.9005, val loss = 0.8985\n",
      "Epoch 96: train loss = 0.8995, val loss = 0.8977\n",
      "Epoch 97: train loss = 0.8989, val loss = 0.8968\n",
      "Epoch 98: train loss = 0.8980, val loss = 0.8960\n",
      "Epoch 99: train loss = 0.8973, val loss = 0.8952\n",
      "Epoch 100: train loss = 0.8964, val loss = 0.8944\n",
      "Epoch 101: train loss = 0.8956, val loss = 0.8936\n",
      "Epoch 102: train loss = 0.8948, val loss = 0.8929\n",
      "Epoch 103: train loss = 0.8940, val loss = 0.8921\n",
      "Epoch 104: train loss = 0.8933, val loss = 0.8913\n",
      "Epoch 105: train loss = 0.8924, val loss = 0.8906\n",
      "Epoch 106: train loss = 0.8917, val loss = 0.8899\n",
      "Epoch 107: train loss = 0.8911, val loss = 0.8892\n",
      "Epoch 108: train loss = 0.8903, val loss = 0.8884\n",
      "Epoch 109: train loss = 0.8895, val loss = 0.8877\n",
      "Epoch 110: train loss = 0.8889, val loss = 0.8871\n",
      "Epoch 111: train loss = 0.8882, val loss = 0.8864\n",
      "Epoch 112: train loss = 0.8876, val loss = 0.8857\n",
      "Epoch 113: train loss = 0.8870, val loss = 0.8851\n",
      "Epoch 114: train loss = 0.8862, val loss = 0.8845\n",
      "Epoch 115: train loss = 0.8856, val loss = 0.8839\n",
      "Epoch 116: train loss = 0.8848, val loss = 0.8832\n",
      "Epoch 117: train loss = 0.8844, val loss = 0.8826\n",
      "Epoch 118: train loss = 0.8838, val loss = 0.8821\n",
      "Epoch 119: train loss = 0.8832, val loss = 0.8815\n",
      "Epoch 120: train loss = 0.8828, val loss = 0.8810\n",
      "Epoch 121: train loss = 0.8821, val loss = 0.8804\n",
      "Epoch 122: train loss = 0.8816, val loss = 0.8799\n",
      "Epoch 123: train loss = 0.8812, val loss = 0.8794\n",
      "Epoch 124: train loss = 0.8806, val loss = 0.8789\n",
      "Epoch 125: train loss = 0.8802, val loss = 0.8784\n",
      "Epoch 126: train loss = 0.8797, val loss = 0.8780\n",
      "Epoch 127: train loss = 0.8792, val loss = 0.8775\n",
      "Epoch 128: train loss = 0.8788, val loss = 0.8771\n",
      "Epoch 129: train loss = 0.8782, val loss = 0.8766\n",
      "Epoch 130: train loss = 0.8778, val loss = 0.8762\n",
      "Epoch 131: train loss = 0.8774, val loss = 0.8758\n",
      "Epoch 132: train loss = 0.8770, val loss = 0.8754\n",
      "Epoch 133: train loss = 0.8766, val loss = 0.8748\n",
      "Epoch 134: train loss = 0.8760, val loss = 0.8744\n",
      "Epoch 135: train loss = 0.8757, val loss = 0.8740\n",
      "Epoch 136: train loss = 0.8752, val loss = 0.8735\n",
      "Epoch 137: train loss = 0.8747, val loss = 0.8731\n",
      "Epoch 138: train loss = 0.8743, val loss = 0.8727\n",
      "Epoch 139: train loss = 0.8739, val loss = 0.8723\n",
      "Epoch 140: train loss = 0.8735, val loss = 0.8719\n",
      "Epoch 141: train loss = 0.8732, val loss = 0.8715\n",
      "Epoch 142: train loss = 0.8727, val loss = 0.8711\n",
      "Epoch 143: train loss = 0.8722, val loss = 0.8704\n",
      "Epoch 144: train loss = 0.8717, val loss = 0.8700\n",
      "Epoch 145: train loss = 0.8712, val loss = 0.8695\n",
      "Epoch 146: train loss = 0.8707, val loss = 0.8690\n",
      "Epoch 147: train loss = 0.8701, val loss = 0.8685\n",
      "Epoch 148: train loss = 0.8696, val loss = 0.8681\n",
      "Epoch 149: train loss = 0.8691, val loss = 0.8676\n",
      "Epoch 150: train loss = 0.8684, val loss = 0.8665\n",
      "Epoch 151: train loss = 0.8671, val loss = 0.8653\n",
      "Epoch 152: train loss = 0.8660, val loss = 0.8643\n",
      "Epoch 153: train loss = 0.8651, val loss = 0.8636\n",
      "Epoch 154: train loss = 0.8644, val loss = 0.8630\n",
      "Epoch 155: train loss = 0.8639, val loss = 0.8625\n",
      "Epoch 156: train loss = 0.8636, val loss = 0.8621\n",
      "Epoch 157: train loss = 0.8630, val loss = 0.8617\n",
      "Epoch 158: train loss = 0.8626, val loss = 0.8613\n",
      "Epoch 159: train loss = 0.8622, val loss = 0.8610\n",
      "Epoch 160: train loss = 0.8620, val loss = 0.8607\n",
      "Epoch 161: train loss = 0.8617, val loss = 0.8604\n",
      "Epoch 162: train loss = 0.8613, val loss = 0.8601\n",
      "Epoch 163: train loss = 0.8611, val loss = 0.8598\n",
      "Epoch 164: train loss = 0.8609, val loss = 0.8597\n",
      "Epoch 165: train loss = 0.8606, val loss = 0.8594\n",
      "Epoch 166: train loss = 0.8603, val loss = 0.8592\n",
      "Epoch 167: train loss = 0.8601, val loss = 0.8590\n",
      "Epoch 168: train loss = 0.8600, val loss = 0.8588\n",
      "Epoch 169: train loss = 0.8598, val loss = 0.8586\n",
      "Epoch 170: train loss = 0.8596, val loss = 0.8585\n",
      "Epoch 171: train loss = 0.8595, val loss = 0.8582\n",
      "Epoch 172: train loss = 0.8592, val loss = 0.8581\n",
      "Epoch 173: train loss = 0.8591, val loss = 0.8579\n",
      "Epoch 174: train loss = 0.8590, val loss = 0.8578\n",
      "Epoch 175: train loss = 0.8587, val loss = 0.8576\n",
      "Epoch 176: train loss = 0.8587, val loss = 0.8575\n",
      "Epoch 177: train loss = 0.8586, val loss = 0.8574\n",
      "Epoch 178: train loss = 0.8583, val loss = 0.8573\n",
      "Epoch 179: train loss = 0.8583, val loss = 0.8571\n",
      "Epoch 180: train loss = 0.8583, val loss = 0.8569\n",
      "Epoch 181: train loss = 0.8581, val loss = 0.8569\n",
      "Epoch 182: train loss = 0.8579, val loss = 0.8567\n",
      "Epoch 183: train loss = 0.8577, val loss = 0.8566\n",
      "Epoch 184: train loss = 0.8577, val loss = 0.8565\n",
      "Epoch 185: train loss = 0.8575, val loss = 0.8564\n",
      "Epoch 186: train loss = 0.8575, val loss = 0.8562\n",
      "Epoch 187: train loss = 0.8573, val loss = 0.8561\n",
      "Epoch 188: train loss = 0.8572, val loss = 0.8561\n",
      "Epoch 189: train loss = 0.8572, val loss = 0.8559\n",
      "Epoch 190: train loss = 0.8570, val loss = 0.8558\n",
      "Epoch 191: train loss = 0.8569, val loss = 0.8557\n",
      "Epoch 192: train loss = 0.8568, val loss = 0.8556\n",
      "Epoch 193: train loss = 0.8567, val loss = 0.8556\n",
      "Epoch 194: train loss = 0.8567, val loss = 0.8554\n",
      "Epoch 195: train loss = 0.8567, val loss = 0.8554\n",
      "Epoch 196: train loss = 0.8565, val loss = 0.8553\n",
      "Epoch 197: train loss = 0.8563, val loss = 0.8552\n",
      "Epoch 198: train loss = 0.8562, val loss = 0.8552\n",
      "Epoch 199: train loss = 0.8561, val loss = 0.8551\n",
      "Epoch 200: train loss = 0.8560, val loss = 0.8550\n"
     ]
    }
   ],
   "source": [
    "def generate_sequence(model, start_token, eos_token_id, id_to_token, max_len=50, device=\"cpu\"):\n",
    "    model.eval()\n",
    "    generated = [start_token]\n",
    "    \n",
    "    for _ in range(max_len):\n",
    "        input_tensor = torch.tensor(generated, dtype=torch.long, device=device).unsqueeze(1)  # [seq_len, 1]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = model(input_tensor)  # [seq_len, 1, vocab_size]\n",
    "        \n",
    "        next_token_logits = output[-1, 0]  # last token's logits: [vocab_size]\n",
    "        next_token = torch.argmax(F.softmax(next_token_logits, dim=-1)).item()\n",
    "        \n",
    "        generated.append(next_token)\n",
    "        \n",
    "        if next_token == eos_token_id:\n",
    "            break\n",
    "\n",
    "    # Convert token IDs back to characters\n",
    "    decoded = [id_to_token[token_id] for token_id in generated if token_id in id_to_token]\n",
    "    return decodeddef train_epoch(model, loader, optimizer, loss_fn):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for x, y in loader:\n",
    "        logits = model(x)  # (batch, seq_len, vocab_size)\n",
    "\n",
    "        loss = loss_fn(logits.view(-1, logits.size(-1)), y.view(-1))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "\n",
    "def eval_epoch(model, loader, loss_fn):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            logits = model(x)\n",
    "            loss = loss_fn(logits.view(-1, logits.size(-1)), y.view(-1))\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "vocab_size = 5\n",
    "\n",
    "model = Transformer(max_seq_len=512, dictionary_size=vocab_size)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-3)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(200):\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, loss_fn)\n",
    "    val_loss = eval_epoch(model, val_loader, loss_fn)\n",
    "    print(f\"Epoch {epoch+1}: train loss = {train_loss:.4f}, val loss = {val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958e18e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated flip-flop sentence: r0i0i0i0i0i0i0i0i0i0i0i0i0i0i0i0i0i0i0i0i0i0i0i0i0i\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "def generate_sequence(model, start_token, id_to_token, max_len=50):\n",
    "    model.eval()\n",
    "    generated = [start_token]\n",
    "    \n",
    "    for _ in range(max_len):\n",
    "        input_tensor = torch.tensor(generated, dtype=torch.long).unsqueeze(1)  # [seq_len, 1]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = model(input_tensor)  # [seq_len, 1, vocab_size]\n",
    "        \n",
    "        next_token_logits = output[-1, 0]  # last token's logits: [vocab_size]\n",
    "        next_token = torch.argmax(F.softmax(next_token_logits, dim=-1)).item()\n",
    "        \n",
    "        generated.append(next_token)\n",
    "\n",
    "    # Convert token IDs back to characters\n",
    "    decoded = [id_to_token[token_id] for token_id in generated if token_id in id_to_token]\n",
    "    return decoded\n",
    "\n",
    "\n",
    "id_to_token = {v: k for k, v in flip_flop_dict.items()}\n",
    "\n",
    "\n",
    "generated_sentence = generate_sequence(model, start_token, id_to_token=id_to_token)\n",
    "\n",
    "print(\"Generated flip-flop sentence:\", \"\".join(generated_sentence))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
