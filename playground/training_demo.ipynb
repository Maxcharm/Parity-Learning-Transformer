{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1df401aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/han/miniconda3/envs/fl/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "\n",
    "dataset = datasets.load_dataset(\"synthseq/flipflop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8bcaf8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "flip_flop_dict = {'0': 0, \"1\": 1, \"w\": 2,\"r\": 3, \"i\": 4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b983737b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def tokenize_raw(batch):\n",
    "    tokenized = [[flip_flop_dict[char] for char in s] for s in batch[\"text\"]]\n",
    "    return {\"tokens\": torch.tensor(tokenized, dtype=torch.int64)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "62c82a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.set_transform(tokenize_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ef1fb141",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_73661/1061149626.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  tokens = torch.tensor(tokens, dtype=torch.long)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class NextTokenDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, hf_dataset):\n",
    "        self.data = []\n",
    "        for item in hf_dataset:\n",
    "            tokens = item[\"tokens\"]\n",
    "            tokens = torch.tensor(tokens, dtype=torch.long)\n",
    "            x = tokens[:-1]\n",
    "            y = tokens[1:]\n",
    "            self.data.append((x, y))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx] \n",
    "\n",
    "train_small = dataset[\"train\"].select(range(2000))\n",
    "val_small = dataset[\"val\"].select(range(200))\n",
    "train_dataset = NextTokenDataset(train_small)\n",
    "val_dataset = NextTokenDataset(val_small)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b5074720",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class Sinusoidal_Embedding(nn.Module):\n",
    "    def __init__(self, embed_dim, max_len=512):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, embed_dim)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, embed_dim, 2) * (-math.log(10000.0) / embed_dim)\n",
    "        )\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer(\"pe\", pe) \n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.pe[:x.size(1)].unsqueeze(0).expand(x.size(0), -1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "208b4181",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            max_seq_len:int,\n",
    "            dictionary_size:int,\n",
    "            num_attn_layer:int=2,\n",
    "            num_attn_heads:int=2,\n",
    "            attn_dim:int=4,\n",
    "            \n",
    "        ):\n",
    "        super().__init__()\n",
    "        self.word_embedding = nn.Embedding(\n",
    "            num_embeddings=dictionary_size,\n",
    "            embedding_dim=attn_dim,\n",
    "            )\n",
    "        self.positional_embedding = Sinusoidal_Embedding(\n",
    "            embed_dim=attn_dim,\n",
    "            max_len=max_seq_len\n",
    "        )\n",
    "        self.attention_layers = [\n",
    "            nn.MultiheadAttention(\n",
    "                embed_dim=attn_dim,\n",
    "                num_heads=num_attn_heads,\n",
    "                batch_first=True,\n",
    "            ) \n",
    "            for _ in range(num_attn_layer)\n",
    "        ]\n",
    "        self.classification = [\n",
    "            nn.Linear(in_features=attn_dim, out_features=attn_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=attn_dim, out_features=dictionary_size),\n",
    "        ]\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        shape of x should be: (batch_size, seq_len)\n",
    "        \"\"\"\n",
    "        seq_len = x.size()[1]\n",
    "        word_emb = self.word_embedding(x)\n",
    "        pos_emb = self.positional_embedding(x)\n",
    "        emb = word_emb + pos_emb\n",
    "\n",
    "        attn_mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool().to(x.device)\n",
    "\n",
    "        for attn_layer in self.attention_layers:\n",
    "            emb, _ = attn_layer(emb, emb, emb, attn_mask=attn_mask)\n",
    "\n",
    "        \n",
    "        for layer in self.classification:\n",
    "            emb = layer(emb)\n",
    "\n",
    "        return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ea55b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train loss = 1.5250, val loss = 1.5243\n",
      "Epoch 2: train loss = 1.5248, val loss = 1.5240\n",
      "Epoch 3: train loss = 1.5237, val loss = 1.5210\n",
      "Epoch 4: train loss = 1.5162, val loss = 1.5051\n",
      "Epoch 5: train loss = 1.4749, val loss = 1.4438\n",
      "Epoch 6: train loss = 1.4271, val loss = 1.4105\n",
      "Epoch 7: train loss = 1.3991, val loss = 1.3868\n",
      "Epoch 8: train loss = 1.3789, val loss = 1.3699\n",
      "Epoch 9: train loss = 1.3642, val loss = 1.3570\n",
      "Epoch 10: train loss = 1.3525, val loss = 1.3458\n",
      "Epoch 11: train loss = 1.3408, val loss = 1.3316\n",
      "Epoch 12: train loss = 1.3156, val loss = 1.2963\n",
      "Epoch 13: train loss = 1.2799, val loss = 1.2647\n",
      "Epoch 14: train loss = 1.2555, val loss = 1.2456\n",
      "Epoch 15: train loss = 1.2379, val loss = 1.2292\n",
      "Epoch 16: train loss = 1.2224, val loss = 1.2146\n",
      "Epoch 17: train loss = 1.2085, val loss = 1.2015\n",
      "Epoch 18: train loss = 1.1959, val loss = 1.1894\n",
      "Epoch 19: train loss = 1.1843, val loss = 1.1783\n",
      "Epoch 20: train loss = 1.1734, val loss = 1.1678\n",
      "Epoch 21: train loss = 1.1633, val loss = 1.1578\n",
      "Epoch 22: train loss = 1.1537, val loss = 1.1485\n",
      "Epoch 23: train loss = 1.1445, val loss = 1.1397\n",
      "Epoch 24: train loss = 1.1359, val loss = 1.1313\n",
      "Epoch 25: train loss = 1.1279, val loss = 1.1235\n",
      "Epoch 26: train loss = 1.1202, val loss = 1.1162\n",
      "Epoch 27: train loss = 1.1132, val loss = 1.1092\n",
      "Epoch 28: train loss = 1.1064, val loss = 1.1027\n",
      "Epoch 29: train loss = 1.1001, val loss = 1.0965\n",
      "Epoch 30: train loss = 1.0940, val loss = 1.0906\n",
      "Epoch 31: train loss = 1.0883, val loss = 1.0849\n",
      "Epoch 32: train loss = 1.0826, val loss = 1.0794\n",
      "Epoch 33: train loss = 1.0773, val loss = 1.0742\n",
      "Epoch 34: train loss = 1.0723, val loss = 1.0692\n",
      "Epoch 35: train loss = 1.0674, val loss = 1.0644\n",
      "Epoch 36: train loss = 1.0626, val loss = 1.0595\n",
      "Epoch 37: train loss = 1.0575, val loss = 1.0546\n",
      "Epoch 38: train loss = 1.0529, val loss = 1.0501\n",
      "Epoch 39: train loss = 1.0485, val loss = 1.0457\n",
      "Epoch 40: train loss = 1.0441, val loss = 1.0413\n",
      "Epoch 41: train loss = 1.0396, val loss = 1.0367\n",
      "Epoch 42: train loss = 1.0350, val loss = 1.0320\n",
      "Epoch 43: train loss = 1.0301, val loss = 1.0270\n",
      "Epoch 44: train loss = 1.0252, val loss = 1.0219\n"
     ]
    }
   ],
   "source": [
    "def train_epoch(model, loader, optimizer, loss_fn):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for x, y in loader:\n",
    "        logits = model(x)  # (batch, seq_len, vocab_size)\n",
    "\n",
    "        loss = loss_fn(logits.view(-1, logits.size(-1)), y.view(-1))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "\n",
    "def eval_epoch(model, loader, loss_fn):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            logits = model(x)\n",
    "            loss = loss_fn(logits.view(-1, logits.size(-1)), y.view(-1))\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "vocab_size = 5\n",
    "\n",
    "model = Transformer(max_seq_len=512, dictionary_size=vocab_size)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-3)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(200):\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, loss_fn)\n",
    "    val_loss = eval_epoch(model, val_loader, loss_fn)\n",
    "    print(f\"Epoch {epoch+1}: train loss = {train_loss:.4f}, val loss = {val_loss:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
