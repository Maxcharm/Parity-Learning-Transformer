{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e011904",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fa8418b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparseDNABinaryDataset(Dataset):\n",
    "    def __init__(self, n_samples=1000, seq_len=10):\n",
    "        self.bases = ['A', 'C', 'G', 'T']\n",
    "        self.seq_len = seq_len\n",
    "        self.data = []\n",
    "\n",
    "        for _ in range(n_samples):\n",
    "            seq = [random.choice(self.bases) for _ in range(seq_len)]\n",
    "            label = 1 if seq[2] == 'A' or seq[4] == 'T' else 0\n",
    "            self.data.append((seq, label))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        seq, label = self.data[idx]\n",
    "        # Encode DNA bases as integers\n",
    "        base2idx = {'A': 0, 'C': 1, 'G': 2, 'T': 3}\n",
    "        encoded_seq = torch.tensor([base2idx[base] for base in seq], dtype=torch.long)\n",
    "        label = torch.tensor(label, dtype=torch.float)\n",
    "        return encoded_seq, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eecfb8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c74f78b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, seq_len, d_model=32, nhead=2, num_layers=1, ff_dim=64):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, seq_len, d_model))\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=ff_dim, batch_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(d_model * seq_len, ff_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x) + self.pos_embedding[:, :x.size(1), :]\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = x.flatten(start_dim=1)  # flatten all tokens\n",
    "        out = self.classifier(x).squeeze(1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9267a490",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics import HingeLoss\n",
    "\n",
    "def train_model(model, dataloader, epochs=50):\n",
    "    model.train()\n",
    "    loss_fn = HingeLoss(task='binary')\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for x, y in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            pred = model(x)\n",
    "            loss = loss_fn(pred, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1} | Loss: {total_loss/len(dataloader):.4f}\")\n",
    "\n",
    "\n",
    "def evaluate_model(model, dataloader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in dataloader:\n",
    "            out = model(x)\n",
    "            preds = (out>=0.5)\n",
    "            correct += (preds == y).sum().item()\n",
    "            total += y.size(0)\n",
    "\n",
    "    print(f\"Accuracy: {correct / total * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4c774c9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training on DNA Classification ---\n",
      "Epoch 1 | Loss: 1.0089\n",
      "Epoch 2 | Loss: 0.7813\n",
      "Epoch 3 | Loss: 0.5993\n",
      "Epoch 4 | Loss: 0.5735\n",
      "Epoch 5 | Loss: 0.5841\n",
      "Epoch 6 | Loss: 0.5778\n",
      "Epoch 7 | Loss: 0.5893\n",
      "Epoch 8 | Loss: 0.5775\n",
      "Epoch 9 | Loss: 0.5833\n",
      "Epoch 10 | Loss: 0.5774\n",
      "Epoch 11 | Loss: 0.5861\n",
      "Epoch 12 | Loss: 0.5831\n",
      "Epoch 13 | Loss: 0.5890\n",
      "Epoch 14 | Loss: 0.5802\n",
      "Epoch 15 | Loss: 0.5860\n",
      "Epoch 16 | Loss: 0.5743\n",
      "Epoch 17 | Loss: 0.5889\n",
      "Epoch 18 | Loss: 0.5831\n",
      "Epoch 19 | Loss: 0.5831\n",
      "Epoch 20 | Loss: 0.5831\n",
      "Epoch 21 | Loss: 0.5801\n",
      "Epoch 22 | Loss: 0.5860\n",
      "Epoch 23 | Loss: 0.5801\n",
      "Epoch 24 | Loss: 0.5772\n",
      "Epoch 25 | Loss: 0.5742\n",
      "Epoch 26 | Loss: 0.5742\n",
      "Epoch 27 | Loss: 0.5830\n",
      "Epoch 28 | Loss: 0.5918\n",
      "Epoch 29 | Loss: 0.5830\n",
      "Epoch 30 | Loss: 0.5772\n",
      "Epoch 31 | Loss: 0.5830\n",
      "Epoch 32 | Loss: 0.5801\n",
      "Epoch 33 | Loss: 0.5830\n",
      "Epoch 34 | Loss: 0.5684\n",
      "Epoch 35 | Loss: 0.5830\n",
      "Epoch 36 | Loss: 0.5860\n",
      "Epoch 37 | Loss: 0.5859\n",
      "Epoch 38 | Loss: 0.5830\n",
      "Epoch 39 | Loss: 0.5801\n",
      "Epoch 40 | Loss: 0.5859\n",
      "Epoch 41 | Loss: 0.5742\n",
      "Epoch 42 | Loss: 0.5889\n",
      "Epoch 43 | Loss: 0.5889\n",
      "Epoch 44 | Loss: 0.5713\n",
      "Epoch 45 | Loss: 0.5801\n",
      "Epoch 46 | Loss: 0.5830\n",
      "Epoch 47 | Loss: 0.5801\n",
      "Epoch 48 | Loss: 0.5801\n",
      "Epoch 49 | Loss: 0.5830\n",
      "Epoch 50 | Loss: 0.5772\n",
      "Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "dna_dataset = SparseDNABinaryDataset()\n",
    "dna_loader = DataLoader(dna_dataset, batch_size=32, shuffle=True)\n",
    "model_dna = TransformerClassifier(vocab_size=4, seq_len=10)\n",
    "print(\"\\n--- Training on DNA Classification ---\")\n",
    "train_model(model_dna, dna_loader)\n",
    "evaluate_model(model_dna, dna_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1cabb1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
